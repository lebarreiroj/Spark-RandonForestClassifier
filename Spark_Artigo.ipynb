{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3af1ec-a2fb-4f76-bf86-2ed3e7bbbd9e",
   "metadata": {},
   "source": [
    "# Artigo no Linkedin\n",
    "\n",
    "## Classificação usando Spark - RandonForestClassifier\n",
    "\n",
    "### Dados\n",
    "\n",
    "Dados de Acidentes da Polícia Rodoviária Federal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7f95b-3cde-41d9-935d-faff1d993556",
   "metadata": {},
   "source": [
    "# 1. TRATAMENTO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557081dc-452e-4418-b0a0-20ca4e9562c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#  Imports\n",
    "\n",
    "# \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# utilidades\n",
    "from datetime import date, datetime, timedelta\n",
    "#\n",
    "# retirar mensagens de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d52f91-dfda-473d-8bf3-23200d4c6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#  Imports\n",
    "\n",
    "# Importar o PySpark\n",
    "import pyspark\n",
    "\n",
    "# \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# SQL\n",
    "\n",
    "from pyspark.sql.functions import when, col, trim, countDistinct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e27672-dc13-4749-8d3e-ecb75b7dbe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/02 18:55:34 WARN Utils: Your hostname, Luiss-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.188 instead (on interface en0)\n",
      "23/03/02 18:55:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/luisjesus/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/02 18:55:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/02 18:55:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Sessão Pyspark - SparkSession\n",
    "\n",
    "# Sessão\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[4]') \\\n",
    "    .appName(\"ClassifierCrash\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457dc8e6-4ac0-452f-8119-422dff9562f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Quantidade de anos de dados de acidentes a serem processados\n",
    "# 1 = 2016 | 2 = 2016 e 2017 | ... | 7 = 2016 até 2021\n",
    "\n",
    "qtd_anos_processamento = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b911f4-9a22-43b5-a6bf-4cc22185c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#  Definição do Schema - Campos dos CSVs que serão carregados\n",
    "# Observação: target não é campo do CSV\n",
    "\n",
    "acidente_schema = StructType([\n",
    "         StructField(\"id\",IntegerType(),True),\n",
    "         StructField(\"data_inversa\",StringType(),True),\n",
    "         StructField(\"dia_semana\",StringType(),True),\n",
    "         StructField(\"horario\",StringType(),True),\n",
    "         StructField(\"uf\",StringType(),True),\n",
    "         StructField(\"br\",IntegerType(),True),\n",
    "         StructField(\"km\",StringType(),True),\n",
    "         StructField(\"municipio\",StringType(),True),\n",
    "         StructField(\"causa_acidente\",StringType(),True),\n",
    "         StructField(\"tipo_acidente\",StringType(),True),\n",
    "         StructField(\"classificacao_acidente\",StringType(),True),\n",
    "         StructField(\"fase_dia\",StringType(),True),\n",
    "         StructField(\"sentido_via\",StringType(),True),\n",
    "         StructField(\"condicao_metereologica\",StringType(),True),\n",
    "         StructField(\"tipo_pista\",StringType(),True),\n",
    "         StructField(\"tracado_via\",StringType(),True),\n",
    "         StructField(\"uso_solo\",StringType(),True),\n",
    "         StructField(\"pessoas\",IntegerType(),True),\n",
    "         StructField(\"mortos\",IntegerType(),True),\n",
    "         StructField(\"feridos_leves\",IntegerType(),True),\n",
    "         StructField(\"feridos_graves\",IntegerType(),True),\n",
    "         StructField(\"ilesos\",IntegerType(),True),\n",
    "         StructField(\"ignorados\",IntegerType(),True),\n",
    "         StructField(\"feridos\",IntegerType(),True),\n",
    "         StructField(\"veiculos\",IntegerType(),True),\n",
    "         StructField(\"target\",IntegerType(),True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e9df24-8aaf-401d-9cd2-ccdb23ecdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#  Procedures e Funções\n",
    "\n",
    "def _carrega_spark_dataframe(_ano, df=None, mySchema=None, _separador=\",\", _enconding=\"latin1\"):\n",
    "    print(f\"Início da carga do arquivo de acidentes de {_ano}....\", datetime.today())\n",
    "    \n",
    "    # Carregar o arquivo para o spark dataframe\n",
    "    dftmp = spark.read.format(\"csv\").schema(mySchema).option(\"header\",\"True\").option(\"sep\",_separador).option(\"encoding\",_enconding).load(f\"./dados/datatran{_ano}.csv\")\n",
    "    # Verificar se foi passado dataframe\n",
    "    if df==None:\n",
    "        df = dftmp\n",
    "    else:\n",
    "        df = df.union(dftmp)\n",
    "    \n",
    "    # print após carga\n",
    "    print(f\"Fim da carga do arquivo de acidentes de {_ano}....\", datetime.today())\n",
    "    print(\"Total de registros carregados...\",dftmp.count())\n",
    "    print(\"Total de registros acumulados...\",df.count())\n",
    "    # delete de dataframe temporário\n",
    "    del dftmp\n",
    "    # retornar o dataframe concatenado\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4397618-aa38-4ee3-99ec-2bac9ac7a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início da carga do arquivo de acidentes de 2016.... 2023-03-02 18:55:37.773111\n",
      "Fim da carga do arquivo de acidentes de 2016.... 2023-03-02 18:55:39.888112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros carregados... 96363\n",
      "Total de registros acumulados... 96363\n",
      "Início da carga do arquivo de acidentes de 2017.... 2023-03-02 18:55:44.834036\n",
      "Fim da carga do arquivo de acidentes de 2017.... 2023-03-02 18:55:44.908935\n",
      "Total de registros carregados... 89567\n",
      "Total de registros acumulados... 185930\n",
      "Início da carga do arquivo de acidentes de 2018.... 2023-03-02 18:55:46.339525\n",
      "Fim da carga do arquivo de acidentes de 2018.... 2023-03-02 18:55:46.417866\n",
      "Total de registros carregados... 69332\n",
      "Total de registros acumulados... 255262\n",
      "Início da carga do arquivo de acidentes de 2019.... 2023-03-02 18:55:47.744995\n",
      "Fim da carga do arquivo de acidentes de 2019.... 2023-03-02 18:55:47.812930\n",
      "Total de registros carregados... 67556\n",
      "Total de registros acumulados... 322818\n",
      "Início da carga do arquivo de acidentes de 2020.... 2023-03-02 18:55:49.545980\n",
      "Fim da carga do arquivo de acidentes de 2020.... 2023-03-02 18:55:49.609612\n",
      "Total de registros carregados... 63576\n",
      "Total de registros acumulados... 386394\n",
      "Início da carga do arquivo de acidentes de 2021.... 2023-03-02 18:55:51.245240\n",
      "Fim da carga do arquivo de acidentes de 2021.... 2023-03-02 18:55:51.320120\n",
      "Total de registros carregados... 64539\n",
      "Total de registros acumulados... 450933\n",
      "Início da carga do arquivo de acidentes de 2022.... 2023-03-02 18:55:52.815152\n",
      "Fim da carga do arquivo de acidentes de 2022.... 2023-03-02 18:55:52.880217\n",
      "Total de registros carregados... 64447\n",
      "Total de registros acumulados... 515380\n"
     ]
    }
   ],
   "source": [
    "# Realização da carga do arquivos para dataframe\n",
    "# parâmetros: ano dos regitros, dataframe, separador, encoding\n",
    "\n",
    "if qtd_anos_processamento >= 1:\n",
    "    dft = _carrega_spark_dataframe(\"2016\", None, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 2:\n",
    "    dft = _carrega_spark_dataframe(\"2017\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 3:\n",
    "    dft = _carrega_spark_dataframe(\"2018\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 4:\n",
    "    dft = _carrega_spark_dataframe(\"2019\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 5:\n",
    "    dft = _carrega_spark_dataframe(\"2020\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 6:\n",
    "    dft = _carrega_spark_dataframe(\"2021\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 7:\n",
    "    dft = _carrega_spark_dataframe(\"2022\", dft, acidente_schema, \";\",\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de42f52-81fc-491c-bb6c-5ebc00673395",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Copiar DataFrame\n",
    "\n",
    "sparkDF = dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb0a17d-2875-4cff-8172-c5db25c0cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#  Fazer limpeza e atualização em campos\n",
    "\n",
    "# Marcar a coluna target - 1 = Acidente Grave | 2 = Acidente não grave\n",
    "sparkDF = sparkDF.withColumn(\"target\", when(sparkDF.mortos >= 1, 1) \\\n",
    "      .when(sparkDF.feridos_graves >=1, 1) \\\n",
    "      .otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a825fb-2eaa-480b-b5b5-6132c5961880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retirada de registros que tem campos nulos ....\n",
      "Total de registros no Dataframe antes da limpeza =  515380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/02 18:55:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 45:==============================================>         (23 + 4) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros no Dataframe após a limpeza =  514449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Retirar os campos com colunas vazias - usar o na.drop()\n",
    "\n",
    "print(\"Retirada de registros que tem campos nulos ....\")\n",
    "print(\"Total de registros no Dataframe antes da limpeza = \", sparkDF.count())\n",
    "sparkDF = sparkDF.na.drop()\n",
    "print(\"Total de registros no Dataframe após a limpeza = \", sparkDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d438ed67-133a-41d6-b9c4-06031291b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retirada de registros de acidentes sem vítimas e ignorados ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros no Dataframe antes da limpeza =  514449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros no Dataframe após a limpeza de 'Ignorados' =  514449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:==================================>                     (17 + 4) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros no Dataframe após a limpeza de 'Sem Vítimas' =  388017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "#  Fazer limpeza e atualização em campos\n",
    "\n",
    "# Filtrar - deixar somente registros com vítimas\n",
    "print(\"Retirada de registros de acidentes sem vítimas e ignorados ....\")\n",
    "print(\"Total de registros no Dataframe antes da limpeza = \", sparkDF.count())\n",
    "sparkDF = sparkDF.filter(col(\"classificacao_acidente\") != 'Ignorados')\n",
    "print(\"Total de registros no Dataframe após a limpeza de 'Ignorados' = \", sparkDF.count())\n",
    "sparkDF = sparkDF.filter(col(\"classificacao_acidente\") != 'Sem Vítimas')\n",
    "\n",
    "print(\"Total de registros no Dataframe após a limpeza de 'Sem Vítimas' = \", sparkDF.count())\n",
    "\n",
    "#[nan 'COM VÍTIMAS FERIDAS' 'SEM VÍTIMAS' 'COM VÍTIMAS FATAIS' 'IGNORADO'\n",
    " \n",
    " \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9770b18f-0cf7-4967-93c0-3b55b4a50f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Colunas categóricas - Lista das colunas\n",
    "\n",
    "categoricalColumns = [ \"dia_semana\"\n",
    "                      #,\"horario\"\n",
    "                      #,\"br\"\n",
    "                      ,\"causa_acidente\"\n",
    "                      ,\"tipo_acidente\"\n",
    "                      ,\"classificacao_acidente\"\n",
    "                      ,\"fase_dia\"\n",
    "                      ,\"sentido_via\"\n",
    "                      ,\"condicao_metereologica\"\n",
    "                      ,\"tipo_pista\"\n",
    "                      ,\"tracado_via\"\n",
    "                      ,\"uso_solo\"\n",
    "                      ,\"pessoas\"\n",
    "                      ,\"veiculos\"\n",
    "                      #,\"hora\"\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f5bfce4-03f0-4883-b852-a59b586a3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Encode dos dados das Colunas categóricas\n",
    "\n",
    "# loop \n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol+\"_encoded\").fit(sparkDF)\n",
    "    sparkDF = stringIndexer.transform(sparkDF)\n",
    "    sparkDF = sparkDF.withColumn(categoricalCol+\"_encoded\", sparkDF[categoricalCol+\"_encoded\"].cast('int'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "334c9fb5-51f3-4115-b288-4ec2a975eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- data_inversa: string (nullable = true)\n",
      " |-- dia_semana: string (nullable = true)\n",
      " |-- horario: string (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- br: integer (nullable = true)\n",
      " |-- km: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- causa_acidente: string (nullable = true)\n",
      " |-- tipo_acidente: string (nullable = true)\n",
      " |-- classificacao_acidente: string (nullable = true)\n",
      " |-- fase_dia: string (nullable = true)\n",
      " |-- sentido_via: string (nullable = true)\n",
      " |-- condicao_metereologica: string (nullable = true)\n",
      " |-- tipo_pista: string (nullable = true)\n",
      " |-- tracado_via: string (nullable = true)\n",
      " |-- uso_solo: string (nullable = true)\n",
      " |-- pessoas: integer (nullable = true)\n",
      " |-- mortos: integer (nullable = true)\n",
      " |-- feridos_leves: integer (nullable = true)\n",
      " |-- feridos_graves: integer (nullable = true)\n",
      " |-- ilesos: integer (nullable = true)\n",
      " |-- ignorados: integer (nullable = true)\n",
      " |-- feridos: integer (nullable = true)\n",
      " |-- veiculos: integer (nullable = true)\n",
      " |-- target: integer (nullable = false)\n",
      " |-- dia_semana_encoded: integer (nullable = true)\n",
      " |-- causa_acidente_encoded: integer (nullable = true)\n",
      " |-- tipo_acidente_encoded: integer (nullable = true)\n",
      " |-- classificacao_acidente_encoded: integer (nullable = true)\n",
      " |-- fase_dia_encoded: integer (nullable = true)\n",
      " |-- sentido_via_encoded: integer (nullable = true)\n",
      " |-- condicao_metereologica_encoded: integer (nullable = true)\n",
      " |-- tipo_pista_encoded: integer (nullable = true)\n",
      " |-- tracado_via_encoded: integer (nullable = true)\n",
      " |-- uso_solo_encoded: integer (nullable = true)\n",
      " |-- pessoas_encoded: integer (nullable = true)\n",
      " |-- veiculos_encoded: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87f0b681-389c-4852-8faf-25a6c5370bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Criar DataFrame com enconded\n",
    "\n",
    "encoded_df =  sparkDF.select(\"dia_semana_encoded\"\n",
    "                      ,\"causa_acidente_encoded\"\n",
    "                      ,\"tipo_acidente_encoded\"\n",
    "                      ,\"classificacao_acidente_encoded\"\n",
    "                      ,\"fase_dia_encoded\"\n",
    "                      ,\"sentido_via_encoded\"\n",
    "                      ,\"condicao_metereologica_encoded\"\n",
    "                      ,\"tipo_pista_encoded\"\n",
    "                      ,\"tracado_via_encoded\"\n",
    "                      ,\"uso_solo_encoded\"\n",
    "                      ,\"pessoas_encoded\"\n",
    "                      ,\"veiculos_encoded\"\n",
    "                      #,\"hora_encoded\"\n",
    "                      ,\"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c544d87-62fe-4d6f-a01d-1a18d10c0ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "388017"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mostrar 5 primeiras linhas\n",
    "encoded_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31500ba6-3860-4728-99e2-ab08dff16aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Preparar features extraction\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureAssembler = VectorAssembler(inputCols=[\"dia_semana_encoded\"\n",
    "                      ,\"causa_acidente_encoded\"\n",
    "                      ,\"tipo_acidente_encoded\"\n",
    "                      ,\"classificacao_acidente_encoded\"\n",
    "                      ,\"fase_dia_encoded\"\n",
    "                      ,\"sentido_via_encoded\"\n",
    "                      ,\"condicao_metereologica_encoded\"\n",
    "                      ,\"tipo_pista_encoded\"\n",
    "                      ,\"tracado_via_encoded\"\n",
    "                      ,\"uso_solo_encoded\"\n",
    "                      ,\"pessoas_encoded\"\n",
    "                      ,\"veiculos_encoded\"\n",
    "                      #,\"hora_encoded\"\n",
    "                                             ],outputCol=\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b22de25-07a5-4e86-898d-2a695a7c6708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dia_semana_encoded: integer (nullable = true)\n",
      " |-- causa_acidente_encoded: integer (nullable = true)\n",
      " |-- tipo_acidente_encoded: integer (nullable = true)\n",
      " |-- classificacao_acidente_encoded: integer (nullable = true)\n",
      " |-- fase_dia_encoded: integer (nullable = true)\n",
      " |-- sentido_via_encoded: integer (nullable = true)\n",
      " |-- condicao_metereologica_encoded: integer (nullable = true)\n",
      " |-- tipo_pista_encoded: integer (nullable = true)\n",
      " |-- tracado_via_encoded: integer (nullable = true)\n",
      " |-- uso_solo_encoded: integer (nullable = true)\n",
      " |-- pessoas_encoded: integer (nullable = true)\n",
      " |-- veiculos_encoded: integer (nullable = true)\n",
      " |-- labels: integer (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Assembler \n",
    "\n",
    "output = featureAssembler.transform(encoded_df)\n",
    "#output2 = labelsAssembler.transform(encoded_df)\n",
    "\n",
    "output.withColumnRenamed(\"target\",\"labels\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd6bda7d-b5d0-4f4b-b1aa-f75a7a1cbfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|(12,[0,1,4,6,9],[...|     0|\n",
      "|(12,[0,1,4,6,7,9]...|     0|\n",
      "|[9.0,5.0,20.0,0.0...|     1|\n",
      "|[9.0,3.0,14.0,0.0...|     0|\n",
      "|[9.0,3.0,3.0,0.0,...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Mostrar o resultado do assembler \n",
    "\n",
    "output.select(\"features\",\"target\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e68e0f-37a2-49d6-bc84-246ad585bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "# using custom schema to make sure \"species\" column is \"string\" datatype. \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType\n",
    "\n",
    "#udf_result = StructType([StructField('target',StringType())])\n",
    "udf_result = StructType([StructField('target',IntegerType())])\n",
    "\n",
    "target_dict = {'Não Grave': '0', 'Grave': '1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d04cd8be-878d-49f4-89bc-7061ca95462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def assign_labels(target):\n",
    "    return Row(target_dict[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901a4eb0-ae26-4363-a04b-64de109f0f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dia_semana_encoded: integer (nullable = true)\n",
      " |-- causa_acidente_encoded: integer (nullable = true)\n",
      " |-- tipo_acidente_encoded: integer (nullable = true)\n",
      " |-- classificacao_acidente_encoded: integer (nullable = true)\n",
      " |-- fase_dia_encoded: integer (nullable = true)\n",
      " |-- sentido_via_encoded: integer (nullable = true)\n",
      " |-- condicao_metereologica_encoded: integer (nullable = true)\n",
      " |-- tipo_pista_encoded: integer (nullable = true)\n",
      " |-- tracado_via_encoded: integer (nullable = true)\n",
      " |-- uso_solo_encoded: integer (nullable = true)\n",
      " |-- pessoas_encoded: integer (nullable = true)\n",
      " |-- veiculos_encoded: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- labels: struct (nullable = true)\n",
      " |    |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assign_labels_udf = F.udf(assign_labels, udf_result)\n",
    "output.withColumn('labels', assign_labels_udf('target')).drop('target').printSchema()\n",
    "# since \"species\" column is labeled under \"labels\", droped species column to avoid redundancy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e5fb7-94e0-4c09-a8a3-74f47ca5cccd",
   "metadata": {},
   "source": [
    "# 4. SPARK - RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8442c8e0-aa24-423d-bfd3-a0da2f8b036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "m = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cf53622-878a-4e77-b4ff-13a308399941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : RandomForestClassifier .... 2023-03-02 18:56:42.274547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop  : RandomForestClassifier .... 2023-03-02 18:57:09.263407\n",
      "Start : RandomForestClassifier Transform .... 2023-03-02 18:57:09.263746\n",
      "Stop  : RandomForestClassifier Transform .... 2023-03-02 18:57:09.370358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : RandomForestClassifier .... 2023-03-02 18:57:14.293908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop  : RandomForestClassifier .... 2023-03-02 18:57:34.556063\n",
      "Start : RandomForestClassifier Transform .... 2023-03-02 18:57:34.556247\n",
      "Stop  : RandomForestClassifier Transform .... 2023-03-02 18:57:34.670888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : RandomForestClassifier .... 2023-03-02 18:57:38.342312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop  : RandomForestClassifier .... 2023-03-02 18:57:57.179783\n",
      "Start : RandomForestClassifier Transform .... 2023-03-02 18:57:57.180160\n",
      "Stop  : RandomForestClassifier Transform .... 2023-03-02 18:57:57.310763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : RandomForestClassifier .... 2023-03-02 18:58:01.065718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop  : RandomForestClassifier .... 2023-03-02 18:58:20.406903\n",
      "Start : RandomForestClassifier Transform .... 2023-03-02 18:58:20.407244\n",
      "Stop  : RandomForestClassifier Transform .... 2023-03-02 18:58:20.489025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : RandomForestClassifier .... 2023-03-02 18:58:25.400701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop  : RandomForestClassifier .... 2023-03-02 18:58:43.821245\n",
      "Start : RandomForestClassifier Transform .... 2023-03-02 18:58:43.821589\n",
      "Stop  : RandomForestClassifier Transform .... 2023-03-02 18:58:43.918550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Separar em treino e teste\n",
    "\n",
    "resultado = []\n",
    "l_start_fit_spark = []\n",
    "l_stop_fit_spark = []\n",
    "l_start_predict_spark = []\n",
    "l_stop_predict_spark = []\n",
    "l_acuracia = []\n",
    "l_total_registros = []\n",
    "l_rodada = []\n",
    "\n",
    "total_registros = sparkDF.count()\n",
    "\n",
    "# classificador\n",
    "#rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'target', numTrees = 500, maxDepth = 5)\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'target')\n",
    "\n",
    "# loop repetindo o total usado para o teste não spark\n",
    "for i in range(m):\n",
    "    # split\n",
    "    train, test = output.randomSplit([0.7, 0.3])\n",
    "    \n",
    "    #train.show(5)\n",
    "    \n",
    "    #Training Model\n",
    "    start_fit_spark =  datetime.today()\n",
    "    print(\"Start : RandomForestClassifier ....\",start_fit_spark)\n",
    "    rfModel = rf.fit(train)\n",
    "    stop_fit_spark =  datetime.today()\n",
    "    print(\"Stop  : RandomForestClassifier ....\", datetime.today())\n",
    "\n",
    "    \n",
    "    #Prediction\n",
    "    start_predict_spark =  datetime.today()\n",
    "    print(\"Start : RandomForestClassifier Transform ....\",start_predict_spark)\n",
    "    predictions = rfModel.transform(test)\n",
    "    stop_predict_spark =  datetime.today() \n",
    "    print(\"Stop  : RandomForestClassifier Transform ....\",stop_predict_spark)\n",
    "    \n",
    "    \n",
    "    #Evaluating the performance\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    evaluator.setLabelCol(\"target\")\n",
    "    evaluator.setPredictionCol(\"prediction\")\n",
    "    acucacia = evaluator.evaluate(predictions)\n",
    "\n",
    "    # guardar resultado\n",
    "    #rodada = [start_fit, stop_fit, start_proba, stop_proba, start_predict, stop_predict, acuracia_teste]\n",
    "    #resultado.append(rodada)\n",
    "    l_start_fit_spark.append(start_fit_spark)\n",
    "    l_stop_fit_spark.append(stop_fit_spark)\n",
    "    #l_start_proba.append(start_proba)\n",
    "    #l_stop_proba.append(stop_proba)\n",
    "    l_start_predict_spark.append(start_predict_spark)\n",
    "    l_stop_predict_spark.append(stop_predict_spark)\n",
    "    l_acuracia.append(acucacia)\n",
    "    l_total_registros.append(total_registros)\n",
    "    l_rodada.append(qtd_anos_processamento)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88198ead-8d8f-41cb-a0c9-c076c189b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado = pd.DataFrame(zip(l_rodada, l_total_registros, l_start_fit_spark, l_stop_fit_spark, l_start_predict_spark, l_stop_predict_spark, l_acuracia),\n",
    "                            columns = ['rodada','total_registros', 'start_fit', 'stop_fit', 'start_predict', 'stop_predict', 'acuracia'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0b58f2e-395c-47c9-8eaf-29e74ffecf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "df_resultado['tempo_fit'] = df_resultado['stop_fit'] - df_resultado['start_fit']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e47c89b7-a77b-40ac-ba83-899bd530cc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rodada</th>\n",
       "      <th>total_registros</th>\n",
       "      <th>start_fit</th>\n",
       "      <th>stop_fit</th>\n",
       "      <th>start_predict</th>\n",
       "      <th>stop_predict</th>\n",
       "      <th>acuracia</th>\n",
       "      <th>tempo_fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>388017</td>\n",
       "      <td>2023-03-02 18:56:42.274547</td>\n",
       "      <td>2023-03-02 18:57:09.263356</td>\n",
       "      <td>2023-03-02 18:57:09.263746</td>\n",
       "      <td>2023-03-02 18:57:09.370358</td>\n",
       "      <td>0.701159</td>\n",
       "      <td>0 days 00:00:26.988809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>388017</td>\n",
       "      <td>2023-03-02 18:57:14.293908</td>\n",
       "      <td>2023-03-02 18:57:34.556006</td>\n",
       "      <td>2023-03-02 18:57:34.556247</td>\n",
       "      <td>2023-03-02 18:57:34.670888</td>\n",
       "      <td>0.705421</td>\n",
       "      <td>0 days 00:00:20.262098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>388017</td>\n",
       "      <td>2023-03-02 18:57:38.342312</td>\n",
       "      <td>2023-03-02 18:57:57.179708</td>\n",
       "      <td>2023-03-02 18:57:57.180160</td>\n",
       "      <td>2023-03-02 18:57:57.310763</td>\n",
       "      <td>0.700493</td>\n",
       "      <td>0 days 00:00:18.837396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>388017</td>\n",
       "      <td>2023-03-02 18:58:01.065718</td>\n",
       "      <td>2023-03-02 18:58:20.406856</td>\n",
       "      <td>2023-03-02 18:58:20.407244</td>\n",
       "      <td>2023-03-02 18:58:20.489025</td>\n",
       "      <td>0.700597</td>\n",
       "      <td>0 days 00:00:19.341138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>388017</td>\n",
       "      <td>2023-03-02 18:58:25.400701</td>\n",
       "      <td>2023-03-02 18:58:43.821198</td>\n",
       "      <td>2023-03-02 18:58:43.821589</td>\n",
       "      <td>2023-03-02 18:58:43.918550</td>\n",
       "      <td>0.700028</td>\n",
       "      <td>0 days 00:00:18.420497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rodada  total_registros                  start_fit  \\\n",
       "0       7           388017 2023-03-02 18:56:42.274547   \n",
       "1       7           388017 2023-03-02 18:57:14.293908   \n",
       "2       7           388017 2023-03-02 18:57:38.342312   \n",
       "3       7           388017 2023-03-02 18:58:01.065718   \n",
       "4       7           388017 2023-03-02 18:58:25.400701   \n",
       "\n",
       "                    stop_fit              start_predict  \\\n",
       "0 2023-03-02 18:57:09.263356 2023-03-02 18:57:09.263746   \n",
       "1 2023-03-02 18:57:34.556006 2023-03-02 18:57:34.556247   \n",
       "2 2023-03-02 18:57:57.179708 2023-03-02 18:57:57.180160   \n",
       "3 2023-03-02 18:58:20.406856 2023-03-02 18:58:20.407244   \n",
       "4 2023-03-02 18:58:43.821198 2023-03-02 18:58:43.821589   \n",
       "\n",
       "                stop_predict  acuracia              tempo_fit  \n",
       "0 2023-03-02 18:57:09.370358  0.701159 0 days 00:00:26.988809  \n",
       "1 2023-03-02 18:57:34.670888  0.705421 0 days 00:00:20.262098  \n",
       "2 2023-03-02 18:57:57.310763  0.700493 0 days 00:00:18.837396  \n",
       "3 2023-03-02 18:58:20.489025  0.700597 0 days 00:00:19.341138  \n",
       "4 2023-03-02 18:58:43.918550  0.700028 0 days 00:00:18.420497  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362ccca-f0c9-4ac2-a415-dd181b44a192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
